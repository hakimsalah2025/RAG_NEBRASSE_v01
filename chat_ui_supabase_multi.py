# -*- coding: utf-8 -*-
"""
ğŸ’¬ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© â€“ Ù…Ø´Ø±ÙˆØ¹ Ù†Ø¨Ø±Ø§Ø³ (Ù†Ø³Ø®Ø© Supabase Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª + Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¯Ù„Ø§Ù„ÙŠ ÙˆÙ…Ø±Ø§Ø¬Ø¹)
"""

import streamlit as st
import pg8000
import os
import json
import math
import requests
import textwrap
from dotenv import load_dotenv

# ==================== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ====================
load_dotenv()

DB = dict(
    host=os.getenv("host"),
    port=os.getenv("port"),
    user=os.getenv("user"),
    password=os.getenv("password"),
    dbname=os.getenv("dbname")
)

LM_STUDIO_BASE = "http://127.0.0.1:1234/v1"
EMBED_MODEL = "text-embedding-intfloat-multilingual-e5-large-instruct"
TOP_K = 5
MIN_ACCEPT = 0.8

# ==================== Ø£Ø¯ÙˆØ§Øª Ø¹Ø§Ù…Ø© ====================
def connect_db():
    return pg8000.connect(**DB)

def cosine(a, b):
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return 0.0 if (na == 0 or nb == 0) else dot / (na * nb)

def embed_text(text):
    """ØªÙˆÙ„ÙŠØ¯ ØªØ¶Ù…ÙŠÙ† Ø¹Ø¨Ø± LM Studio"""
    r = requests.post(f"{LM_STUDIO_BASE}/embeddings",
                      json={"model": EMBED_MODEL, "input": text})
    r.raise_for_status()
    return r.json()["data"][0]["embedding"]

def search_chunks(query):
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©"""
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("SELECT book_name, content, start_line, end_line, embedding_vector FROM chunk;")
    rows = cur.fetchall()
    cur.close(); conn.close()

    q_vec = embed_text(query)
    results = []
    for (book_name, content, s, e, emb) in rows:
        score = cosine(q_vec, emb)
        if score >= MIN_ACCEPT:
            results.append({
                "book_name": book_name,
                "content": content,
                "start_line": s,
                "end_line": e,
                "score": score
            })
    results = sorted(results, key=lambda x: x["score"], reverse=True)[:TOP_K]
    return results

# ==================== Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ====================
def fetch_conversations():
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("SELECT id, title FROM conversation ORDER BY id DESC;")
    rows = cur.fetchall()
    cur.close(); conn.close()
    return [{"id": r[0], "title": r[1]} for r in rows]

def create_conversation(title="Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©"):
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("INSERT INTO conversation (title) VALUES (%s) RETURNING id;", (title,))
    cid = cur.fetchone()[0]
    conn.commit(); cur.close(); conn.close()
    return cid

def delete_conversation(conv_id):
    """ğŸ—‘ï¸ Ø­Ø°Ù Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆÙƒÙ„ Ø±Ø³Ø§Ø¦Ù„Ù‡Ø§"""
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("DELETE FROM message WHERE conversation_id = %s;", (conv_id,))
    cur.execute("DELETE FROM conversation WHERE id = %s;", (conv_id,))
    conn.commit(); cur.close(); conn.close()
    st.rerun()

def fetch_messages(conv_id):
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("SELECT role, content FROM message WHERE conversation_id = %s ORDER BY id ASC;", (conv_id,))
    rows = cur.fetchall()
    cur.close(); conn.close()
    return [{"role": r[0], "content": r[1]} for r in rows]

def save_message(conv_id, role, content):
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("INSERT INTO message (conversation_id, role, content) VALUES (%s, %s, %s);",
                (conv_id, role, content))
    conn.commit(); cur.close(); conn.close()

def update_conversation_title(conv_id, new_title):
    """ØªØ­Ø¯ÙŠØ« Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("UPDATE conversation SET title = %s WHERE id = %s;", (new_title, conv_id))
    conn.commit(); cur.close(); conn.close()

# ==================== ÙˆØ§Ø¬Ù‡Ø© Streamlit ====================
st.set_page_config(page_title="ğŸ’¬ Ù†Ø¨Ø±Ø§Ø³ Chat", layout="wide")

st.sidebar.title("ğŸ“š Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª")
convs = fetch_conversations()
st.sidebar.write("Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª:", len(convs))

# Ø²Ø± Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©
if st.sidebar.button("â• Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©"):
    cid = create_conversation()
    st.session_state["conversation_id"] = cid
    st.session_state["messages"] = []
    st.rerun()

# Ø¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ø¹ Ø²Ø± Ø§Ù„Ø­Ø°Ù ğŸ—‘ï¸
for c in convs:
    col1, col2 = st.sidebar.columns([4, 1])
    with col1:
        if st.sidebar.button(c["title"], key=f"conv_{c['id']}"):
            st.session_state["conversation_id"] = c["id"]
            st.session_state["messages"] = fetch_messages(c["id"])
            st.rerun()
    with col2:
        if st.sidebar.button("ğŸ—‘ï¸", key=f"del_{c['id']}"):
            delete_conversation(c["id"])

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
if "conversation_id" not in st.session_state:
    if convs:
        st.session_state["conversation_id"] = convs[0]["id"]
        st.session_state["messages"] = fetch_messages(convs[0]["id"])
    else:
        cid = create_conversation()
        st.session_state["conversation_id"] = cid
        st.session_state["messages"] = []

conv_id = st.session_state["conversation_id"]
messages = st.session_state["messages"]

st.title("ğŸ’¬ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© â€“ Ù…Ø´Ø±ÙˆØ¹ Ù†Ø¨Ø±Ø§Ø³")
st.write("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ³ÙŠØ¬ÙŠØ¨Ùƒ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©.")

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for msg in messages:
    role = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
    st.chat_message(msg["role"], avatar=role).markdown(msg["content"])

# ==================== ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ====================
prompt = st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")

if prompt:
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙˆØ±Ù‹Ø§
    st.chat_message("user", avatar="ğŸ‘¤").markdown(prompt)
    save_message(conv_id, "user", prompt)
    st.session_state["messages"].append({"role": "user", "content": prompt})

    # ğŸ”¹ ØªØ­Ø¯ÙŠØ« Ø§Ø³Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ù† Ø£ÙˆÙ„ Ø³Ø¤Ø§Ù„ ÙÙ‚Ø·
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM message WHERE conversation_id = %s;", (conv_id,))
    count = cur.fetchone()[0]
    cur.close(); conn.close()
    if count == 1:  # Ø£ÙˆÙ„ Ø±Ø³Ø§Ù„Ø©
        title = textwrap.shorten(prompt.strip().replace("\n", " "), width=40, placeholder="â€¦")
        update_conversation_title(conv_id, title)

    # ğŸ” Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©
    ranked = search_chunks(prompt)

    if ranked:
        # 1ï¸âƒ£ Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        context_blocks = []
        for i, r in enumerate(ranked, 1):
            context_blocks.append(
                f"ğŸ”¹ (Ù…Ø±Ø¬Ø¹ {i}) Ù…Ù† ÙƒØªØ§Ø¨ {r['book_name']} â€“ Ø§Ù„Ø£Ø³Ø·Ø± {r['start_line']}â€“{r['end_line']}:\n{r['content']}\n"
            )
        context = "\n".join(context_blocks)

        # 2ï¸âƒ£ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ù„Ø¹Ø±Ø¶Ù‡Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        refs_text = []
        for i, r in enumerate(ranked, 1):
            excerpt = " ".join(r["content"].split()[:25]) + "..."
            refs_text.append(
                f"(Ù…Ø±Ø¬Ø¹ {i}) {r['book_name']} â€“ Ø§Ù„Ø£Ø³Ø·Ø± {r['start_line']}â€“{r['end_line']} â€“ ØªØ´Ø§Ø¨Ù‡: {r['score']*100:.1f}%\n"
                f'Ù…Ù‚ØªØ·Ù: "{excerpt}"\n'
            )
        refs_summary = "\n".join(refs_text)
    else:
        context = "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ Ù…Ø±ØªØ¨Ø·Ø© ÙƒÙØ§ÙŠØ©."
        refs_summary = ""

    # ğŸ”¹ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    from llm_client import generate_answer
    response = generate_answer(prompt, context)

    # ğŸ”¹ Ø¥Ù„Ø­Ø§Ù‚ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø¨Ø§Ù„Ø±Ø¯
    if refs_summary:
        response += "\n\n---\n\nğŸ“– **Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªØ¹Ù…Ù„Ø©:**\n" + refs_summary

    # Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø¯
    st.chat_message("assistant", avatar="ğŸ¤–").markdown(response)
    save_message(conv_id, "assistant", response)
    st.session_state["messages"].append({"role": "assistant", "content": response})
