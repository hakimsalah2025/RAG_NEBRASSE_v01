# -*- coding: utf-8 -*-
"""
chat_ui.py
ğŸ’¬ ÙˆØ§Ø¬Ù‡Ø© Ø¯Ø±Ø¯Ø´Ø© Ø±Ø³ÙˆÙ…ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit â€“ Ù…Ù‡ÙŠØ£Ø© Ù„Ù€ OpenAI GPT-4o-mini
Ø¨Ø®ØµØ§Ø¦Øµ:
- Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¹Ø¨Ø± RAG
- Ø¹ØªØ¨Ø© ØªÙƒÙŠÙÙŠØ© Ø°ÙƒÙŠØ©
- ØªÙ…Ø±ÙŠØ± Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙØ¹Ù„ÙŠØ©
- Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù…Ø±Ù†
- Ø°Ø§ÙƒØ±Ø© Ø­ÙˆØ§Ø±ÙŠØ© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¬Ù„Ø³Ø©
"""

import streamlit as st
import psycopg2
import requests
import json
import math
from llm_client import generate_from_llm
from dotenv import load_dotenv
import os

# ===================== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ¦Ø© =====================
load_dotenv()

# ===================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ =====================
DB = dict(
    host=os.getenv("host"),
    port=os.getenv("port"),
    user=os.getenv("user"),
    password=os.getenv("password"),
    dbname=os.getenv("dbname")
)


EMBED_MODEL = "text-embedding-intfloat-multilingual-e5-large-instruct"
LANGUAGE_HINT = "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©"
TOP_K = 5
TEMPERATURE = 0.2
MAX_TOKENS = 1024

# ===================== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =====================
def connect_db():
    return psycopg2.connect(**DB)

def cosine(a, b):
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return 0.0 if (na == 0 or nb == 0) else dot / (na * nb)

def embed(text):
    """ØªÙˆÙ„ÙŠØ¯ ØªØ¶Ù…ÙŠÙ† Ù…Ø­Ù„ÙŠ (ÙŠÙ…ÙƒÙ† Ù„Ø§Ø­Ù‚Ù‹Ø§ ØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù€ OpenAI embeddings)"""
    r = requests.post(
        "http://127.0.0.1:1234/v1/embeddings",
        json={"model": EMBED_MODEL, "input": text},
    )
    r.raise_for_status()
    return r.json()["data"][0]["embedding"]

def fetch_chunks():
    """Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("""
        SELECT id, book_id, book_name, content, start_line, end_line, embedding_vector
        FROM chunk
        ORDER BY id ASC;
    """)
    rows = cur.fetchall()
    cur.close(); conn.close()
    return [
        dict(id=c, book_id=b, book_name=n, content=t,
             start_line=s, end_line=e, embedding=v)
        for c, b, n, t, s, e, v in rows
    ]

def short_extract(text, max_words=20):
    words = text.split()
    return text if len(words) <= max_words else " ".join(words[:max_words]) + "..."

def save_message(conversation_id, role, content, refs=None):
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO message (conversation_id, role, content, references_json)
        VALUES (%s, %s, %s, %s)
    """, (conversation_id, role, content, json.dumps(refs) if refs else None))
    conn.commit()
    cur.close(); conn.close()

def ensure_conversation():
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO conversation (title, message_count)
        VALUES (%s, %s)
        RETURNING id;
    """, ("Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ù† Streamlit", 0))
    cid = cur.fetchone()[0]
    conn.commit()
    cur.close(); conn.close()
    return cid

# ===================== Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ (Ø¹ØªØ¨Ø© ØªÙƒÙŠÙÙŠØ©) =====================
def search_chunks(query):
    q_vec = embed(query)
    chunks = fetch_chunks()
    thresholds = [0.80, 0.70, 0.60]
    for threshold in thresholds:
        results = []
        for c in chunks:
            s = cosine(q_vec, c["embedding"])
            if s >= threshold:
                results.append({**c, "score": s})
        if results:
           
            return sorted(results, key=lambda x: x["score"], reverse=True)[:TOP_K]
    st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ÙƒØ§ÙÙŠØ© Ø­ØªÙ‰ Ø¨Ø£Ø¯Ù†Ù‰ Ø¹ØªØ¨Ø© (0.60).")
    return []

# ===================== ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ø¨Ø± OpenAI GPT-4o-mini =====================
def generate_answer(query, ranked):
    """
    ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¹ØªÙ…Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙØ¹Ù„ÙŠØ©
    + Ø¯Ø¹Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø­ÙˆØ§Ø±ÙŠØ© (Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¬Ù„Ø³Ø©)
    """
    # Ø¯Ø§Ù„Ø© Ù„ØªÙ‚ØµÙŠØ± Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    def _clip(text, max_chars=900):
        text = " ".join(text.split())
        return text if len(text) <= max_chars else text[:max_chars].rsplit(" ", 1)[0] + "..."

    # Ø¨Ù†Ø§Ø¡ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
    refs_text = []
    for i, r in enumerate(ranked, 1):
        excerpt = _clip(r["content"], 900)
        refs_text.append(
            f"(Ù…Ø±Ø¬Ø¹ {i}) ÙƒØªØ§Ø¨: {r['book_name']} â€” Ø§Ù„Ø£Ø³Ø·Ø± {r['start_line']}â€“{r['end_line']} â€” ØªØ´Ø§Ø¨Ù‡: {r['score']*100:.1f}%\n"
            f'Ù†Øµ Ø§Ù„Ù…Ù‚Ø·Ø¹: """{excerpt}"""\n'
        )
    sources = "\n".join(refs_text) if refs_text else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø±Ø§Ø¬Ø¹ ÙƒØ§ÙÙŠØ©."

    # === Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­ÙˆØ§Ø±ÙŠ (Ø¢Ø®Ø± 6 Ø±Ø³Ø§Ø¦Ù„) ===
    history = []
    for msg in st.session_state.messages[-6:]:
        history.append(f"{msg['role']}: {msg['content']}")
    history_text = "\n".join(history)

    # Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø§Ù„Ù…Ø±Ù†
    prompt = f"""
Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
{history_text}

Ø£Ù†Øª Ø¨Ø§Ø­Ø« Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙˆØªØ³ØªØ®Ø¯Ù… {LANGUAGE_HINT}.
Ø§Ø¹ØªÙ…Ø¯ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù…ÙŠØ© Ù…ÙˆØ¬Ø²Ø©ØŒ Ù…Ø¹ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ Ø£Ø¯Ù„Ø© ØºÙŠØ± Ù…Ø¨Ø§Ø´Ø±Ø©.

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
1ï¸âƒ£ Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹.
2ï¸âƒ£ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø±Ø¨Ø· Ø¨ÙŠÙ† Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ù‚Ø·Ø¹ Ù„ØªÙƒÙˆÙŠÙ† ÙÙƒØ±Ø© Ù…ØªÙƒØ§Ù…Ù„Ø©.
3ï¸âƒ£ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ØªØªØ¶Ù…Ù† Ø¯Ù„Ø§Ø¦Ù„ Ø¬Ø²Ø¦ÙŠØ© Ø£Ùˆ Ù…ØªÙØ±Ù‚Ø©ØŒ ÙØ§Ø³ØªÙ†ØªØ¬ Ù…Ù†Ù‡Ø§ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ø£Ù‚Ø±Ø¨ Ù„Ù„Ø³Ø¤Ø§Ù„.
4ï¸âƒ£ Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø£ÙŠ Ø¯Ù„Ø§Ù„Ø© Ø¥Ø·Ù„Ø§Ù‚Ù‹Ø§ Ø¨Ø¹Ø¯ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ØŒ ÙˆØ¶Ù‘Ø­ Ø°Ù„Ùƒ Ø¨Ø¥ÙŠØ¬Ø§Ø².
5ï¸âƒ£ Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨Ù‹Ø§ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠÙ‹Ø§ ÙˆØ§Ø¶Ø­Ù‹Ø§ ÙˆÙ…ØªÙ…Ø§Ø³ÙƒÙ‹Ø§.
6ï¸âƒ£ Ø¶Ø¹ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ Ø¨ÙŠÙ† Ù‚ÙˆØ³ÙŠÙ† Ù…Ø«Ù„ (Ù…Ø±Ø¬Ø¹ 1).

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ:
{query}

Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…ØªØ§Ø­Ø©:
{sources}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
""".strip()

    try:
        answer = generate_from_llm(prompt)
    except Exception as e:
        answer = f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ OpenAI: {e}"

    return answer

# ===================== ÙˆØ§Ø¬Ù‡Ø© Streamlit =====================
st.set_page_config(page_title="Ø¯Ø±Ø¯Ø´Ø© Ù†Ø¨Ø±Ø§Ø³", layout="centered")
st.markdown(
    """
    <style>
    body {direction: rtl; text-align: right;}
    .stTextInput label {font-weight: bold;}
    </style>
    """,
    unsafe_allow_html=True,
)

st.title("ğŸ¤– ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© â€“ Ù…Ø´Ø±ÙˆØ¹ Ù†Ø¨Ø±Ø§Ø³ (Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ù€ OpenAI GPT-4o-mini)")
st.write("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ³ÙŠØ¬ÙŠØ¨Ùƒ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙƒØªØ¨Ùƒ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­ÙˆØ§Ø±.")

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
if "conversation_id" not in st.session_state:
    st.session_state.conversation_id = ensure_conversation()
if "messages" not in st.session_state:
    st.session_state.messages = []

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for msg in st.session_state.messages:
    st.chat_message("user" if msg["role"] == "user" else "assistant",
                    avatar="ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–").markdown(msg["content"])

# Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ
prompt = st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")

if prompt:
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user", avatar="ğŸ‘¤").markdown(prompt)

    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    try:
        ranked = search_chunks(prompt)
    except Exception as e:
        ranked = []
        st.error(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {e}")

    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    try:
        answer = generate_answer(prompt, ranked)
    except Exception as e:
        answer = f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {e}"

    # Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
    refs_text = ""
    if ranked and len(ranked) > 0:
        refs_text = "\n\n---\n\nğŸ“– **Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªØ¹Ù…Ù„Ø©:**\n"
        for i, r in enumerate(ranked, 1):
            refs_text += (
                f"- (Ù…Ø±Ø¬Ø¹ {i}) **{r['book_name']}**  \n"
                f"  â€¢ Ø§Ù„Ø£Ø³Ø·Ø±: {r['start_line']}â€“{r['end_line']}  \n"
                f"  â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {r['score']*100:.1f}%  \n"
                f"  â€¢ Ù…Ù‚ØªØ·Ù: â€œ{short_extract(r['content'], 20)}â€\n\n"
            )

    full_answer = answer + refs_text

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯
    st.session_state.messages.append({"role": "assistant", "content": full_answer})
    st.chat_message("assistant", avatar="ğŸ¤–").markdown(full_answer)

    # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    try:
        refs_payload = [
            {"book_name": r["book_name"], "similarity": round(r["score"]*100, 2),
             "excerpt": short_extract(r["content"]), "verified": True}
            for r in ranked
        ]
        save_message(st.session_state.conversation_id, "user", prompt)
        save_message(st.session_state.conversation_id, "assistant", full_answer, refs_payload)
    except Exception as e:
        st.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
