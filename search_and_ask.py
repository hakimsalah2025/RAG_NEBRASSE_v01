# -*- coding: utf-8 -*-
"""
search_and_ask.py  â€“  Ø§Ù„Ø¥ØµØ¯Ø§Ø± 2 (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©)
- ÙŠØ¹Ù…Ù„ Ù…Ø¹ LM Studio Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¹Ø¨Ø± /v1/completions
- ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ multilingual E5 large Ù„Ù„ØªØ¶Ù…ÙŠÙ† (ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø¯Ù‚Ø©)
- ÙŠØªØ¶Ù…Ù† ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¯Ù„Ø§Ù„ÙŠØ© Ù„Ù„Ø¨Ø­Ø« (Query Expansion + Lower Threshold)
- ÙŠØ³ØªØ®Ø±Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø£Ø¯Ù‚ ÙˆØ£ÙƒØ«Ø± Ø§Ø±ØªØ¨Ø§Ø·Ø§Ù‹ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
"""

import os
import sys
import math
import psycopg2
import requests
from textwrap import shorten

# ===================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ =====================
DB = dict(
    host="localhost",
    port=5432,
    user="postgres",
    password="13@04@1971",
    dbname="nebras_rag",
)

LM_STUDIO_BASE = "http://127.0.0.1:1234/v1"

# Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
CHAT_MODEL = "mistralai/mistral-7b-instruct-v0.3"
EMBED_MODEL = "text-embedding-intfloat-multilingual-e5-large-instruct"  # âœ… Ù†Ù…ÙˆØ°Ø¬ Ø¯Ù„Ø§Ù„ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø«
TOP_K = 5           # Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
MIN_ACCEPT = 0.55   # ØªØ®ÙÙŠØ¶ Ø­Ø¯ Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù„ØªÙˆØ³ÙŠØ¹ Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
MAX_TOKENS = 512
TEMPERATURE = 0.2
TIMEOUT = 180


# ===================== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =====================
def cosine(a, b):
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙˆÙ†ÙŠ"""
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x*y for x, y in zip(a, b))
    na = math.sqrt(sum(x*x for x in a))
    nb = math.sqrt(sum(y*y for y in b))
    if na == 0 or nb == 0:
        return 0.0
    return dot / (na * nb)


def embed(text, embed_model):
    """ØªÙˆÙ„ÙŠØ¯ ØªØ¶Ù…ÙŠÙ† Ø¹Ø¨Ø± LM Studio"""
    r = requests.post(f"{LM_STUDIO_BASE}/embeddings",
                      json={"model": embed_model, "input": text},
                      timeout=TIMEOUT)
    r.raise_for_status()
    data = r.json()
    return data["data"][0]["embedding"]


def fetch_chunks():
    """Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    conn = psycopg2.connect(**DB)
    cur = conn.cursor()
    cur.execute("""
        SELECT id, book_id, book_name, content, embedding_vector
        FROM Chunk
        ORDER BY id ASC
    """)
    rows = cur.fetchall()
    cur.close(); conn.close()
    return [dict(id=c, book_id=b, book_name=n, content=t, embedding=v)
            for c,b,n,t,v in rows]


def short_extract(text, max_words=20):
    """Ø§Ù‚ØªØ·Ø§Ù Ù…Ù‚ØªØ·Ù Ù…Ø®ØªØµØ±"""
    words = text.split()
    return text if len(words) <= max_words else " ".join(words[:max_words]) + "..."


def verify_quote_in_chunk(quote, chunk_text):
    """ØªØ­Ù‚Ù‚ Ø­Ø±ÙÙŠ"""
    q = quote.strip().strip('"').rstrip("â€¦").rstrip("...")
    return q in chunk_text


def build_prompt(query, ranked):
    """Ø¨Ù†Ø§Ø¡ prompt Ù„Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ /v1/completions"""
    lines = []
    for i, r in enumerate(ranked, 1):
        excerpt = short_extract(r["content"], 20)
        lines.append(f"[Ù…Ø±Ø¬Ø¹ {i}] ÙƒØªØ§Ø¨: {r['book_name']}\n"
                     f'Ù…Ù‚ØªØ·Ù: "{excerpt}"\n'
                     f"Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {r['score']*100:.1f}%\n")
    sources = "\n".join(lines) if lines else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ØµØ§Ø¯Ø± ÙƒØ§ÙÙŠØ©."

    instruction = (
        "Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ¬Ø²Ø© ÙˆÙ…ØªÙ…Ø§Ø³ÙƒØ© ØªÙˆØ¶Ù‘Ø­ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±ØŒ "
        "Ù…Ø¹ ØªØ¶Ù…ÙŠÙ† Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ø­Ø±ÙÙŠØ© Ù‚ØµÙŠØ±Ø© (â‰¤20 ÙƒÙ„Ù…Ø©) Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ Ù…Ø¹ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ "
        "Ù…Ø«Ù„ (Ù…Ø±Ø¬Ø¹ 1)ØŒ Ø«Ù… Ø£Ø®ØªÙ… Ø¨Ù‚Ø³Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©."
    )

    prompt = f"{instruction}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {query}\n\nØ§Ù„Ù…ØµØ§Ø¯Ø±:\n{sources}\n\n"
    # ØªØ¬Ù†Ù‘Ø¨ Ø£Ù‚ÙˆØ§Ø³ Jinja
    return prompt.replace("{", "(").replace("}", ")")


def chat_with_completions(prompt, chat_model):
    """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ LM Studio Ø¹Ø¨Ø± /v1/completions"""
    payload = {
        "model": chat_model,
        "prompt": prompt,
        "max_tokens": MAX_TOKENS,
        "temperature": TEMPERATURE,
    }
    r = requests.post(f"{LM_STUDIO_BASE}/completions", json=payload, timeout=TIMEOUT)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0].get("text", "").strip()


def ask(query):
    """ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
    print(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: {query}\n")
    print(f"ğŸ§© Chat Model: {CHAT_MODEL}")
    print(f"ğŸ§© Embed Model: {EMBED_MODEL}\n")

    # ğŸŒ ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¯Ù„Ø§Ù„ÙŠÙ‹Ø§ Ù„ØªÙ‚ÙˆÙŠØ© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒØªØ¨
    query += " Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„ØªØ­ÙˆÙ„ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ø¹Ù† Ø¨Ø¹Ø¯ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„ØªØ¹Ù„ÙŠÙ… ØªØ·ÙˆÙŠØ± Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„ÙˆØ·Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ"

    # 1ï¸âƒ£ ØªÙˆÙ„ÙŠØ¯ ØªØ¶Ù…ÙŠÙ†
    q_vec = embed(query, EMBED_MODEL)

    # 2ï¸âƒ£ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    chunks = fetch_chunks()
    scored = []
    for c in chunks:
        s = cosine(q_vec, c["embedding"])
        if s >= MIN_ACCEPT:
            scored.append({**c, "score": s})

    # 3ï¸âƒ£ ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    ranked = sorted(scored, key=lambda x: x["score"], reverse=True)[:TOP_K]

    # 4ï¸âƒ£ Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
    if not ranked:
        print("âš ï¸ Ù„Ù… ØªÙØ¹Ø«Ø± Ù…Ù‚Ø§Ø·Ø¹ ÙƒØ§ÙÙŠØ© â‰¥ 55%.\n")
    else:
        print("ğŸ·ï¸ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹:")
        for i, r in enumerate(ranked, 1):
            print(f"- (Ù…Ø±Ø¬Ø¹ {i}) {r['book_name']} â€” ØªØ´Ø§Ø¨Ù‡: {r['score']*100:.1f}%")
            print("  Ù…Ù‚ØªØ·Ù:", shorten(r["content"], width=120, placeholder="â€¦"))
        print()

    # 5ï¸âƒ£ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    try:
        prompt = build_prompt(query, ranked)
        answer = chat_with_completions(prompt, CHAT_MODEL)
    except Exception as e:
        print("âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:", e)
        return

    # 6ï¸âƒ£ Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    print("\nğŸ§  Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n", answer, "\n")

    # 7ï¸âƒ£ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
    print("ğŸ“– Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ (ØªØ­Ù‚Ù‚ Ø­Ø±ÙÙŠ):")
    if not ranked:
        print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø±Ø§Ø¬Ø¹ ÙƒØ§ÙÙŠØ©."); return
    for i, r in enumerate(ranked, 1):
        excerpt = short_extract(r["content"], 20)
        verified = "âœ“" if verify_quote_in_chunk(excerpt, r["content"]) else "âœ—"
        print(f"(Ù…Ø±Ø¬Ø¹ {i}) ÙƒØªØ§Ø¨: {r['book_name']} â€” ØªØ´Ø§Ø¨Ù‡: {r['score']*100:.1f}% â€” Ù…ØªØ­Ù‚Ù‚: {verified}")
        print(f'Ù…Ù‚ØªØ·Ù: "{excerpt}"\n')


# ===================== ØªÙ†ÙÙŠØ° Ù…Ø¨Ø§Ø´Ø± =====================
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Ø§Ø³ØªØ®Ø¯Ù…:\n  python search_and_ask.py \"Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§\"")
        sys.exit(0)
    query = " ".join(sys.argv[1:])
    ask(query)
