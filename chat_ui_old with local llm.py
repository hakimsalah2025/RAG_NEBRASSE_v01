# -*- coding: utf-8 -*-
"""
chat_ui.py
ğŸ’¬ ÙˆØ§Ø¬Ù‡Ø© Ø¯Ø±Ø¯Ø´Ø© Ø±Ø³ÙˆÙ…ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit â€“ Ù…Ù‡ÙŠØ£Ø© Ù„Ù€ Qwen2.5-7B-Instruct-1M-GGUF
"""

import streamlit as st
import psycopg2
import requests
import json
import math

# ===================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ =====================
DB = dict(
    host="localhost",
    port=5432,
    user="postgres",
    password="13@04@1971",
    dbname="nebras_rag",
)

LM_STUDIO_BASE = "http://127.0.0.1:1234/v1"
CHAT_MODEL = "Qwen2.5-7B-Instruct-1M-GGUF"
EMBED_MODEL = "text-embedding-intfloat-multilingual-e5-large-instruct"

LANGUAGE_HINT = "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©"
TOP_K = 5
MIN_ACCEPT = 0.8
TEMPERATURE = 0.2
MAX_TOKENS = 1024


# ===================== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =====================
def connect_db():
    return psycopg2.connect(**DB)


def cosine(a, b):
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return 0.0 if (na == 0 or nb == 0) else dot / (na * nb)


def embed(text):
    """ØªÙˆÙ„ÙŠØ¯ ØªØ¶Ù…ÙŠÙ† Ø¹Ø¨Ø± LM Studio"""
    r = requests.post(f"{LM_STUDIO_BASE}/embeddings",
                      json={"model": EMBED_MODEL, "input": text})
    r.raise_for_status()
    return r.json()["data"][0]["embedding"]


def fetch_chunks():
    """Ø¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø®Ø²Ù‘Ù†Ø©"""
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("""
        SELECT id, book_id, book_name, content, start_line, end_line, embedding_vector
        FROM chunk
        ORDER BY id ASC;
    """)
    rows = cur.fetchall()
    cur.close(); conn.close()
    return [dict(id=c, book_id=b, book_name=n, content=t,
                 start_line=s, end_line=e, embedding=v)
            for c, b, n, t, s, e, v in rows]


def short_extract(text, max_words=20):
    words = text.split()
    return text if len(words) <= max_words else " ".join(words[:max_words]) + "..."


def save_message(conversation_id, role, content, refs=None):
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO message (conversation_id, role, content, references_json)
        VALUES (%s, %s, %s, %s)
    """, (conversation_id, role, content, json.dumps(refs) if refs else None))
    conn.commit()
    cur.close(); conn.close()


def ensure_conversation():
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("INSERT INTO conversation (title, message_count) VALUES (%s, %s) RETURNING id;",
                ("Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ù† Streamlit", 0))
    cid = cur.fetchone()[0]
    conn.commit()
    cur.close(); conn.close()
    return cid


def search_chunks(query):
    q_vec = embed(query)
    chunks = fetch_chunks()
    results = []
    for c in chunks:
        s = cosine(q_vec, c["embedding"])
        if s >= MIN_ACCEPT:
            results.append({**c, "score": s})
    return sorted(results, key=lambda x: x["score"], reverse=True)[:TOP_K]


def generate_answer(query, ranked):
    """
    ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ÙÙ‚Ø· â€” ÙŠÙ…Ù†Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
    """
    refs_text = []
    for i, r in enumerate(ranked, 1):
        excerpt = short_extract(r["content"], 25)
        refs_text.append(
            f"(Ù…Ø±Ø¬Ø¹ {i}) ÙƒØªØ§Ø¨: {r['book_name']} â€” Ø§Ù„Ø£Ø³Ø·Ø± {r['start_line']}â€“{r['end_line']} â€” ØªØ´Ø§Ø¨Ù‡: {r['score']*100:.1f}%\n"
            f'Ù…Ù‚ØªØ·Ù: "{excerpt}"\n'
        )
    sources = "\n".join(refs_text) if refs_text else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø±Ø§Ø¬Ø¹ ÙƒØ§ÙÙŠØ©."

    prompt = f"""
Ø£Ù†Øª Ø¨Ø§Ø­Ø« Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙˆØªØ³ØªØ®Ø¯Ù… {LANGUAGE_HINT}.
Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆÙ„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù…Ø¹Ø±ÙØ© Ø®Ø§Ø±Ø¬ÙŠØ©.

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµØ§Ø±Ù…Ø©:
1ï¸âƒ£ Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªØ§Ù„ÙŠØ©.
2ï¸âƒ£ Ù„Ø§ ØªØ¶Ù Ø¢Ø±Ø§Ø¡ Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†ØµÙˆØµ.
3ï¸âƒ£ Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙƒØ§ÙÙŠØ©ØŒ Ù‚Ù„: "Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø©".
4ï¸âƒ£ Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨Ù‹Ø§ Ø¹Ø±Ø¨ÙŠÙ‹Ø§ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠÙ‹Ø§ Ù…ÙˆØ¬Ø²Ù‹Ø§ ÙˆÙˆØ§Ø¶Ø­Ù‹Ø§.
5ï¸âƒ£ Ø¶Ø¹ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ Ø¨ÙŠÙ† Ù‚ÙˆØ³ÙŠÙ† Ù…Ø«Ù„ (Ù…Ø±Ø¬Ø¹ 1).

Ø§Ù„Ø³Ø¤Ø§Ù„:
{query}

Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…ØªØ§Ø­Ø©:
{sources}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
""".strip()

    payload = {
        "model": CHAT_MODEL,
        "prompt": prompt,
        "temperature": TEMPERATURE,
        "max_tokens": MAX_TOKENS
    }

    r = requests.post(f"{LM_STUDIO_BASE}/completions", json=payload)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0].get("text", "").strip()


# ===================== ÙˆØ§Ø¬Ù‡Ø© Streamlit =====================
st.set_page_config(page_title="Ø¯Ø±Ø¯Ø´Ø© Ù†Ø¨Ø±Ø§Ø³", layout="centered")
st.markdown(
    """
    <style>
    body {direction: rtl; text-align: right;}
    .stTextInput label {font-weight: bold;}
    </style>
    """,
    unsafe_allow_html=True,
)

st.title("ğŸ¤– ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© â€“ Ù…Ø´Ø±ÙˆØ¹ Ù†Ø¨Ø±Ø§Ø³")
st.write("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ³ÙŠØ¬ÙŠØ¨Ùƒ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙƒØªØ¨Ùƒ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©.")

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
if "conversation_id" not in st.session_state:
    st.session_state.conversation_id = ensure_conversation()
if "messages" not in st.session_state:
    st.session_state.messages = []

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for msg in st.session_state.messages:
    st.chat_message("user" if msg["role"] == "user" else "assistant",
                    avatar="ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–").markdown(msg["content"])

# ===================== Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ =====================
prompt = st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")

if prompt:
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user", avatar="ğŸ‘¤").markdown(prompt)

    # Ø§Ù„Ø¨Ø­Ø«
    try:
        ranked = search_chunks(prompt)
    except Exception as e:
        ranked = []
        st.error(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {e}")

    # Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    try:
        answer = generate_answer(prompt, ranked)
    except Exception as e:
        answer = f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {e}"

    # Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
    refs_text = ""
    if ranked and len(ranked) > 0:
        refs_text = "\n\n---\n\nğŸ“– **Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªØ¹Ù…Ù„Ø©:**\n"
        for i, r in enumerate(ranked, 1):
            refs_text += (
                f"- (Ù…Ø±Ø¬Ø¹ {i}) **{r['book_name']}**  \n"
                f"  â€¢ Ø§Ù„Ø£Ø³Ø·Ø±: {r['start_line']}â€“{r['end_line']}  \n"
                f"  â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {r['score']*100:.1f}%  \n"
                f"  â€¢ Ù…Ù‚ØªØ·Ù: â€œ{short_extract(r['content'], 20)}â€\n\n"
            )

    full_answer = answer + refs_text

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯
    st.session_state.messages.append({"role": "assistant", "content": full_answer})
    st.chat_message("assistant", avatar="ğŸ¤–").markdown(full_answer)

    # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    try:
        refs_payload = [
            {"book_name": r["book_name"], "similarity": round(r["score"]*100, 2),
             "excerpt": short_extract(r["content"]), "verified": True}
            for r in ranked
        ]
        save_message(st.session_state.conversation_id, "user", prompt)
        save_message(st.session_state.conversation_id, "assistant", full_answer, refs_payload)
    except Exception as e:
        st.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
