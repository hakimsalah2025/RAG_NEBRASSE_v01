# -*- coding: utf-8 -*-
"""
ingest_books_supabase.py
ğŸ“š Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ÙƒØªØ¨ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Supabase Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©
"""

import os
import json
import math
import psycopg2
import requests
from tqdm import tqdm
from dotenv import load_dotenv

# ===================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… =====================
load_dotenv()

DB = dict(
    host=os.getenv("host"),
    port=os.getenv("port"),
    user=os.getenv("user"),
    password=os.getenv("password"),
    dbname=os.getenv("dbname"),
)

# ÙŠÙ…ÙƒÙ† Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù…ÙˆØ¯ÙŠÙ„ OpenAI Ù…Ø¨Ø§Ø´Ø±Ø©
EMBED_MODEL = "text-embedding-intfloat-multilingual-e5-large-instruct"
LM_STUDIO_BASE = "http://127.0.0.1:1234/v1"  # Ø¥Ø°Ø§ Ù„Ù… ØªØ³ØªØ®Ø¯Ù… LM StudioØŒ ÙŠÙ…ÙƒÙ† ØªØ¹Ø·ÙŠÙ„Ù‡ Ù…Ø¤Ù‚ØªÙ‹Ø§

CHUNK_SIZE = 400
OVERLAP = 40  # ØªØ¯Ø§Ø®Ù„ 10%
BOOKS_DIR = "./books"  # ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ ÙƒØªØ¨ .txt Ø¯Ø§Ø®Ù„Ù‡

# ===================== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =====================
def connect_db():
    """Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Supabase"""
    return psycopg2.connect(**DB)


def normalize_arabic(text):
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    return " ".join(text.split())


def embed_text(text):
    """ØªÙˆÙ„ÙŠØ¯ ØªØ¶Ù…ÙŠÙ† â€” Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø¹Ø¨Ø± LM Studio (ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¨Ù€ OpenAI)"""
    try:
        r = requests.post(f"{LM_STUDIO_BASE}/embeddings",
                          json={"model": EMBED_MODEL, "input": text})
        r.raise_for_status()
        return r.json()["data"][0]["embedding"]
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {e}")
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙˆÙØ± LM StudioØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ© Ù…Ø¤Ù‚ØªÙ‹Ø§
        return [0.0] * 768


def chunk_text(content):
    """ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø³Ø·Ø±"""
    lines = [l.strip() for l in content.split("\n") if l.strip()]
    line_count = len(lines)
    words = content.split()
    total_words = len(words)

    chunks = []
    step = CHUNK_SIZE - OVERLAP

    for i in range(0, total_words, step):
        chunk_words = words[i:i + CHUNK_SIZE]
        chunk_text = " ".join(chunk_words)

        # ØªØ­Ø¯ÙŠØ¯ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø³Ø¨Ø©
        start_ratio = i / total_words
        end_ratio = min((i + CHUNK_SIZE) / total_words, 1)
        start_line = int(start_ratio * line_count) + 1
        end_line = int(end_ratio * line_count)

        chunks.append({
            "content": chunk_text,
            "start_line": start_line,
            "end_line": end_line,
        })

    return chunks


def insert_book(conn, name, content, chunk_count, line_count):
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO book (name, type, file_url, line_count, chunk_count, size_mb, content, processing_status)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        RETURNING id;
    """, (name, 'text/plain', '', line_count, chunk_count, 0.0, content, 'completed'))
    book_id = cur.fetchone()[0]
    conn.commit()
    cur.close()
    return book_id


def insert_chunk(conn, book_id, book_name, content, start_line, end_line, embedding):
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO chunk (book_id, book_name, content, start_line, end_line, embedding_vector, embedding_model, embedding_dim)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s);
    """, (book_id, book_name, content, start_line, end_line, embedding, EMBED_MODEL, len(embedding)))
    conn.commit()
    cur.close()


# ===================== Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© =====================
def ingest_book(file_path):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØªØ§Ø¨ ÙˆØ§Ø­Ø¯"""
    book_name = os.path.basename(file_path)
    print(f"\nğŸ“˜ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {book_name}")

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    norm_content = normalize_arabic(content)
    chunks = chunk_text(norm_content)

    conn = connect_db()
    book_id = insert_book(conn, book_name, norm_content, len(chunks), len(content.split("\n")))

    print(f"ğŸ“˜ Ø§Ù„ÙƒØªØ§Ø¨ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(chunks)} Ù…Ù‚Ø§Ø·Ø¹.")
    for c in tqdm(chunks, desc="ğŸ”¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"):
        emb = embed_text(c["content"])
        insert_chunk(conn, book_id, book_name, c["content"], c["start_line"], c["end_line"], emb)

    conn.close()
    print(f"âœ… ØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ÙƒØªØ§Ø¨ '{book_name}' Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ Supabase.")


def main():
    files = [f for f in os.listdir(BOOKS_DIR) if f.endswith(".txt")]
    if not files:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒØªØ¨ ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ ./books")
        return

    for f in files:
        ingest_book(os.path.join(BOOKS_DIR, f))


if __name__ == "__main__":
    main()
